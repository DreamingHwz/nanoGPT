/home/ubuntu/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
Overriding config with config/finetune_poem.py:
import time

out_dir = 'out-poems'
eval_interval = 5
eval_iters = 40
wandb_log = False # feel free to turn on
wandb_project = 'poems'
wandb_run_name = 'ft-' + str(time.time())

dataset = 'poems'
data_dir = "data/poems"
init_from = 'resume'

# only save checkpoints if the validation loss improves
always_save_checkpoint = False

# # match the small model config from train_shakespeare_char.py
# n_layer = 4
# n_head = 2
# n_embd = 384
# dropout = 0.2

# the number of examples per iter:
# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter
# poems has 400000 tokens, so 1 epoch ~= 12.2 iters
batch_size = 1
gradient_accumulation_steps = 32
max_iters = 150

# finetune at constant LR
learning_rate = 3e-5
decay_lr = False
tokens per iteration will be: 32,768
Resuming training from out-poems
number of parameters: 1555.97M
num decayed parameter tensors: 194, with 1,556,609,600 parameters
num non-decayed parameter tensors: 386, with 1,001,600 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 10: train loss 3.6222, val loss 3.7294
iter 10: loss 3.6919, time 56559.70ms, mfu -100.00%
iter 11: loss 3.9838, time 3113.33ms, mfu -100.00%
iter 12: loss 0.8248, time 3151.07ms, mfu -100.00%
iter 13: loss 3.4819, time 3150.41ms, mfu -100.00%
iter 14: loss 1.5674, time 3150.66ms, mfu -100.00%
step 15: train loss 3.4765, val loss 3.8136
iter 15: loss 3.1833, time 5399.74ms, mfu 19.99%
iter 16: loss 3.4776, time 3151.59ms, mfu 21.42%
iter 17: loss 3.7577, time 3152.71ms, mfu 22.70%
iter 18: loss 2.7717, time 3153.35ms, mfu 23.86%
iter 19: loss 3.2925, time 3153.43ms, mfu 24.89%
step 20: train loss 3.4149, val loss 3.4201
iter 20: loss 5.0004, time 5404.61ms, mfu 24.40%
iter 21: loss 3.8464, time 3151.64ms, mfu 25.39%
iter 22: loss 3.7901, time 3154.26ms, mfu 26.27%
iter 23: loss 2.8360, time 3153.09ms, mfu 27.07%
iter 24: loss 3.5740, time 3153.93ms, mfu 27.78%
step 25: train loss 3.2935, val loss 3.4457
iter 25: loss 3.6884, time 5402.02ms, mfu 27.00%
iter 26: loss 3.8998, time 3588.05ms, mfu 27.31%
iter 27: loss 2.6389, time 3154.38ms, mfu 28.00%
iter 28: loss 3.6506, time 3154.03ms, mfu 28.63%
iter 29: loss 3.3535, time 3153.89ms, mfu 29.19%
step 30: train loss 3.2814, val loss 3.2907
iter 30: loss 3.8457, time 5405.06ms, mfu 28.27%
iter 31: loss 4.5092, time 3154.97ms, mfu 28.86%
iter 32: loss 1.7189, time 3153.62ms, mfu 29.40%
iter 33: loss 4.4150, time 3153.72ms, mfu 29.88%
iter 34: loss 3.4952, time 3154.03ms, mfu 30.32%
step 35: train loss 3.2135, val loss 3.2404
iter 35: loss 4.2475, time 5405.71ms, mfu 29.28%
iter 36: loss 3.3345, time 3153.63ms, mfu 29.78%
iter 37: loss 4.2836, time 3153.45ms, mfu 30.22%
iter 38: loss 1.5056, time 3153.50ms, mfu 30.62%
iter 39: loss 4.2346, time 3153.76ms, mfu 30.99%
step 40: train loss 3.1644, val loss 3.2908
iter 40: loss 4.3074, time 5403.84ms, mfu 29.88%
iter 41: loss 2.7643, time 3154.44ms, mfu 30.32%
iter 42: loss 3.8399, time 3153.58ms, mfu 30.71%
iter 43: loss 2.5725, time 3153.35ms, mfu 31.06%
iter 44: loss 3.1946, time 3154.22ms, mfu 31.38%
step 45: train loss 2.9674, val loss 3.3840
iter 45: loss 3.7538, time 5407.88ms, mfu 30.24%
iter 46: loss 3.6566, time 3157.74ms, mfu 30.63%
iter 47: loss 4.4977, time 3155.08ms, mfu 30.99%
iter 48: loss 3.7184, time 3157.44ms, mfu 31.31%
iter 49: loss 3.8084, time 3155.54ms, mfu 31.60%
step 50: train loss 3.2950, val loss 3.0710
iter 50: loss 1.9093, time 5531.84ms, mfu 30.39%
iter 51: loss 4.0284, time 3155.92ms, mfu 30.77%
iter 52: loss 2.3641, time 3160.80ms, mfu 31.11%
iter 53: loss 4.1282, time 3157.36ms, mfu 31.42%
iter 54: loss 2.0100, time 3160.75ms, mfu 31.69%
step 55: train loss 2.9938, val loss 3.3355
iter 55: loss 1.8912, time 5516.22ms, mfu 30.48%
iter 56: loss 3.4518, time 3154.63ms, mfu 30.86%
iter 57: loss 3.0477, time 3155.11ms, mfu 31.19%
iter 58: loss 3.7698, time 3155.53ms, mfu 31.49%
iter 59: loss 3.2851, time 3155.20ms, mfu 31.77%
step 60: train loss 3.3221, val loss 3.3665
iter 60: loss 2.4428, time 5483.74ms, mfu 30.56%
iter 61: loss 4.5209, time 3153.92ms, mfu 30.93%
iter 62: loss 3.1550, time 3157.20ms, mfu 31.25%
iter 63: loss 3.2765, time 3155.80ms, mfu 31.55%
iter 64: loss 3.3345, time 3156.62ms, mfu 31.81%
step 65: train loss 3.0136, val loss 3.4943
iter 65: loss 0.9579, time 5502.57ms, mfu 30.59%
iter 66: loss 4.2897, time 3153.35ms, mfu 30.96%
iter 67: loss 2.7742, time 3153.24ms, mfu 31.29%
iter 68: loss 3.0047, time 3155.42ms, mfu 31.58%
iter 69: loss 4.3475, time 3154.92ms, mfu 31.84%
step 70: train loss 3.1879, val loss 3.3489
iter 70: loss 3.4409, time 5403.42ms, mfu 30.66%
iter 71: loss 3.4012, time 3154.12ms, mfu 31.01%
iter 72: loss 3.4641, time 3152.98ms, mfu 31.34%
iter 73: loss 2.4653, time 3153.06ms, mfu 31.63%
iter 74: loss 3.5648, time 3153.07ms, mfu 31.89%
step 75: train loss 3.2127, val loss 3.1489
iter 75: loss 2.4150, time 5405.12ms, mfu 30.70%
iter 76: loss 3.4052, time 3153.46ms, mfu 31.05%
iter 77: loss 3.2561, time 3152.96ms, mfu 31.37%
iter 78: loss 2.1394, time 3153.29ms, mfu 31.66%
iter 79: loss 2.6358, time 3152.81ms, mfu 31.92%
step 80: train loss 2.8915, val loss 3.3493
iter 80: loss 2.8516, time 5403.27ms, mfu 30.72%
iter 81: loss 2.8499, time 3153.18ms, mfu 31.07%
iter 82: loss 3.4113, time 3152.79ms, mfu 31.39%
iter 83: loss 3.1223, time 3153.91ms, mfu 31.67%
iter 84: loss 3.8632, time 3153.01ms, mfu 31.93%
step 85: train loss 3.1265, val loss 3.2629
iter 85: loss 3.2774, time 5417.82ms, mfu 30.73%
iter 86: loss 1.1429, time 3153.10ms, mfu 31.08%
iter 87: loss 3.3992, time 3152.95ms, mfu 31.40%
iter 88: loss 3.6225, time 3153.06ms, mfu 31.68%
iter 89: loss 3.7010, time 3153.57ms, mfu 31.94%
step 90: train loss 3.1179, val loss 3.4477
iter 90: loss 3.5760, time 5404.10ms, mfu 30.74%
iter 91: loss 3.4271, time 3153.52ms, mfu 31.09%
iter 92: loss 2.1687, time 3154.11ms, mfu 31.40%
iter 93: loss 3.2694, time 3153.35ms, mfu 31.69%
iter 94: loss 2.2793, time 3153.02ms, mfu 31.94%
step 95: train loss 3.0200, val loss 3.2652
iter 95: loss 3.7882, time 5407.63ms, mfu 30.75%
iter 96: loss 2.5336, time 3152.59ms, mfu 31.10%
iter 97: loss 4.2245, time 3153.50ms, mfu 31.41%
iter 98: loss 3.3829, time 3152.84ms, mfu 31.69%
iter 99: loss 3.3786, time 3153.05ms, mfu 31.95%
step 100: train loss 3.1674, val loss 3.4991
iter 100: loss 2.8051, time 5402.42ms, mfu 30.75%
iter 101: loss 4.2850, time 3152.62ms, mfu 31.10%
iter 102: loss 3.2697, time 3152.91ms, mfu 31.41%
iter 103: loss 2.8405, time 3362.18ms, mfu 31.48%
iter 104: loss 3.1916, time 3161.47ms, mfu 31.75%
step 105: train loss 2.9789, val loss 3.3925
iter 105: loss 3.2527, time 5525.45ms, mfu 30.53%
iter 106: loss 2.3885, time 3156.35ms, mfu 30.90%
iter 107: loss 3.4887, time 3153.45ms, mfu 31.23%
iter 108: loss 3.4160, time 3154.45ms, mfu 31.53%
iter 109: loss 3.4631, time 3153.89ms, mfu 31.80%
step 110: train loss 3.0630, val loss 3.4202
iter 110: loss 3.0269, time 5413.85ms, mfu 30.61%
iter 111: loss 0.3042, time 3157.92ms, mfu 30.97%
iter 112: loss 2.9895, time 3154.90ms, mfu 31.30%
iter 113: loss 1.7922, time 3154.59ms, mfu 31.59%
iter 114: loss 3.7650, time 3156.24ms, mfu 31.85%
step 115: train loss 2.8284, val loss 3.3206
iter 115: loss 3.9938, time 5402.44ms, mfu 30.66%
iter 116: loss 3.4208, time 3154.40ms, mfu 31.02%
iter 117: loss 1.1359, time 3154.46ms, mfu 31.34%
iter 118: loss 3.2962, time 3153.66ms, mfu 31.63%
iter 119: loss 2.0394, time 3154.39ms, mfu 31.89%
step 120: train loss 3.0328, val loss 3.3982
iter 120: loss 3.4976, time 5404.90ms, mfu 30.70%
iter 121: loss 3.2589, time 3153.77ms, mfu 31.05%
iter 122: loss 2.8794, time 3154.18ms, mfu 31.37%
iter 123: loss 3.4383, time 3154.09ms, mfu 31.66%
iter 124: loss 4.1607, time 3154.12ms, mfu 31.91%
step 125: train loss 3.0968, val loss 3.1364
iter 125: loss 3.5114, time 5410.95ms, mfu 30.72%
iter 126: loss 4.0356, time 3154.66ms, mfu 31.07%
iter 127: loss 3.5395, time 3154.59ms, mfu 31.38%
iter 128: loss 3.2879, time 3153.95ms, mfu 31.67%
iter 129: loss 3.7639, time 3154.35ms, mfu 31.92%
step 130: train loss 2.8288, val loss 3.4147
iter 130: loss 2.9988, time 5414.00ms, mfu 30.73%
iter 131: loss 3.3494, time 3154.85ms, mfu 31.07%
iter 132: loss 3.7608, time 3154.04ms, mfu 31.39%
iter 133: loss 1.1820, time 3153.95ms, mfu 31.67%
iter 134: loss 2.6480, time 3154.40ms, mfu 31.93%
step 135: train loss 2.9336, val loss 3.5058
iter 135: loss 3.3749, time 5407.06ms, mfu 30.73%
iter 136: loss 1.3266, time 3154.13ms, mfu 31.08%
iter 137: loss 3.7653, time 3154.40ms, mfu 31.40%
iter 138: loss 3.4189, time 3153.97ms, mfu 31.68%
iter 139: loss 3.3684, time 3153.94ms, mfu 31.94%
step 140: train loss 2.9632, val loss 3.3610
iter 140: loss 2.6488, time 5408.37ms, mfu 30.74%
iter 141: loss 3.5637, time 3153.50ms, mfu 31.09%
iter 142: loss 2.9158, time 3153.74ms, mfu 31.40%
iter 143: loss 4.1425, time 3153.92ms, mfu 31.69%
iter 144: loss 3.1722, time 3153.65ms, mfu 31.94%
step 145: train loss 3.0797, val loss 3.3655
iter 145: loss 2.5760, time 5407.74ms, mfu 30.74%
iter 146: loss 3.1557, time 3154.48ms, mfu 31.09%
iter 147: loss 3.3323, time 3154.98ms, mfu 31.40%
iter 148: loss 3.4451, time 3155.69ms, mfu 31.68%
iter 149: loss 2.8451, time 3157.68ms, mfu 31.93%
step 150: train loss 2.8534, val loss 3.3543
iter 150: loss 2.3888, time 5453.80ms, mfu 30.72%
training finished in 9m 22.47s
