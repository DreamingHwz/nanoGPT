/home/ubuntu/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
Overriding config with config/finetune_poem.py:
import time

out_dir = 'out-poems'
eval_interval = 5
eval_iters = 40
wandb_log = False # feel free to turn on
wandb_project = 'poems'
wandb_run_name = 'ft-' + str(time.time())

dataset = 'poems'
data_dir = "data/poems"
init_from = 'resume'

# only save checkpoints if the validation loss improves
always_save_checkpoint = False

# # match the small model config from train_shakespeare_char.py
# n_layer = 4
# n_head = 2
# n_embd = 384
# dropout = 0.2

# the number of examples per iter:
# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter
# poems has 400000 tokens, so 1 epoch ~= 12.2 iters
batch_size = 1
gradient_accumulation_steps = 32
max_iters = 150

# finetune at constant LR
learning_rate = 3e-5
decay_lr = False
tokens per iteration will be: 32,768
Resuming training from out-poems
number of parameters: 1555.97M
num decayed parameter tensors: 194, with 1,556,609,600 parameters
num non-decayed parameter tensors: 386, with 1,001,600 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 10: train loss 3.4474, val loss 3.5919
iter 10: loss 2.7255, time 52003.36ms, mfu -100.00%
iter 11: loss 4.3400, time 3104.51ms, mfu -100.00%
iter 12: loss 2.9677, time 3142.79ms, mfu -100.00%
iter 13: loss 3.5596, time 3143.24ms, mfu -100.00%
iter 14: loss 4.1198, time 3143.50ms, mfu -100.00%
step 15: train loss 3.3702, val loss 3.4293
iter 15: loss 3.5982, time 5392.40ms, mfu 20.02%
iter 16: loss 3.0685, time 3145.49ms, mfu 21.45%
iter 17: loss 4.0439, time 3147.46ms, mfu 22.74%
iter 18: loss 3.0829, time 3147.99ms, mfu 23.89%
iter 19: loss 2.1877, time 3151.75ms, mfu 24.93%
step 20: train loss 3.3949, val loss 3.3818
iter 20: loss 4.3851, time 5446.09ms, mfu 24.42%
iter 21: loss 3.8203, time 3147.82ms, mfu 25.41%
iter 22: loss 4.0199, time 3146.97ms, mfu 26.30%
iter 23: loss 2.9633, time 3147.36ms, mfu 27.10%
iter 24: loss 4.3050, time 3145.59ms, mfu 27.82%
step 25: train loss 3.4471, val loss 3.3771
iter 25: loss 2.8057, time 5427.75ms, mfu 27.03%
iter 26: loss 3.1454, time 3597.76ms, mfu 27.32%
iter 27: loss 3.4248, time 3147.57ms, mfu 28.02%
iter 28: loss 3.8421, time 3145.61ms, mfu 28.65%
iter 29: loss 3.4528, time 3146.17ms, mfu 29.22%
step 30: train loss 3.4325, val loss 3.4570
iter 30: loss 3.0334, time 5392.08ms, mfu 28.30%
iter 31: loss 3.4649, time 3145.73ms, mfu 28.90%
iter 32: loss 3.5637, time 3145.86ms, mfu 29.44%
iter 33: loss 3.9399, time 3146.13ms, mfu 29.93%
iter 34: loss 2.4335, time 3146.26ms, mfu 30.37%
step 35: train loss 3.1656, val loss 3.1321
iter 35: loss 3.7575, time 5392.47ms, mfu 29.33%
iter 36: loss 3.6384, time 3146.19ms, mfu 29.83%
iter 37: loss 2.6584, time 3145.58ms, mfu 30.28%
iter 38: loss 3.9788, time 3145.70ms, mfu 30.68%
iter 39: loss 2.8769, time 3145.60ms, mfu 31.05%
step 40: train loss 3.2436, val loss 3.4498
iter 40: loss 1.0705, time 5393.58ms, mfu 29.95%
iter 41: loss 2.9948, time 3146.49ms, mfu 30.38%
iter 42: loss 1.5075, time 3145.49ms, mfu 30.78%
iter 43: loss 1.2878, time 3145.57ms, mfu 31.13%
iter 44: loss 2.9703, time 3145.46ms, mfu 31.45%
step 45: train loss 3.2942, val loss 3.4897
iter 45: loss 3.4885, time 5398.47ms, mfu 30.30%
iter 46: loss 3.6237, time 3147.36ms, mfu 30.70%
iter 47: loss 3.4994, time 3145.71ms, mfu 31.07%
iter 48: loss 3.3070, time 3145.62ms, mfu 31.39%
iter 49: loss 3.5376, time 3145.61ms, mfu 31.68%
step 50: train loss 3.3674, val loss 3.1582
iter 50: loss 3.2745, time 5389.79ms, mfu 30.52%
iter 51: loss 3.4660, time 3146.10ms, mfu 30.90%
iter 52: loss 3.9519, time 3145.40ms, mfu 31.24%
iter 53: loss 3.5388, time 3144.94ms, mfu 31.55%
iter 54: loss 3.1677, time 3145.59ms, mfu 31.83%
step 55: train loss 3.4005, val loss 3.4311
iter 55: loss 3.9981, time 5393.22ms, mfu 30.65%
iter 56: loss 3.8934, time 3146.45ms, mfu 31.01%
iter 57: loss 3.4627, time 3145.85ms, mfu 31.34%
iter 58: loss 2.7423, time 3145.49ms, mfu 31.64%
iter 59: loss 2.0720, time 3145.34ms, mfu 31.91%
step 60: train loss 3.1962, val loss 3.4563
iter 60: loss 3.2367, time 5393.33ms, mfu 30.72%
iter 61: loss 3.5057, time 3145.10ms, mfu 31.08%
iter 62: loss 3.7312, time 3145.44ms, mfu 31.41%
iter 63: loss 3.0909, time 3146.03ms, mfu 31.70%
iter 64: loss 4.0759, time 3148.66ms, mfu 31.96%
step 65: train loss 3.2389, val loss 3.2817
iter 65: loss 2.0570, time 5399.51ms, mfu 30.76%
iter 66: loss 3.5772, time 3145.79ms, mfu 31.12%
iter 67: loss 3.6639, time 3145.55ms, mfu 31.44%
iter 68: loss 3.8572, time 3145.36ms, mfu 31.72%
iter 69: loss 3.3374, time 3145.66ms, mfu 31.98%
step 70: train loss 3.1421, val loss 3.3415
iter 70: loss 3.8192, time 5392.23ms, mfu 30.79%
iter 71: loss 3.1992, time 3145.59ms, mfu 31.14%
iter 72: loss 3.2992, time 3145.40ms, mfu 31.46%
iter 73: loss 3.0313, time 3145.65ms, mfu 31.75%
iter 74: loss 3.1963, time 3146.00ms, mfu 32.00%
step 75: train loss 3.2794, val loss 3.3801
iter 75: loss 2.0248, time 5393.99ms, mfu 30.80%
iter 76: loss 2.9840, time 3142.45ms, mfu 31.16%
iter 77: loss 2.7509, time 3146.15ms, mfu 31.48%
iter 78: loss 1.5693, time 3145.23ms, mfu 31.76%
iter 79: loss 3.3457, time 3145.81ms, mfu 32.02%
step 80: train loss 3.0774, val loss 3.3271
iter 80: loss 3.2988, time 5390.98ms, mfu 30.82%
iter 81: loss 1.5849, time 3145.09ms, mfu 31.17%
iter 82: loss 2.5722, time 3145.06ms, mfu 31.48%
iter 83: loss 4.3950, time 3145.49ms, mfu 31.77%
iter 84: loss 3.4575, time 3145.34ms, mfu 32.02%
step 85: train loss 3.2623, val loss 3.3024
iter 85: loss 2.4525, time 5390.81ms, mfu 30.82%
iter 86: loss 4.3198, time 3145.36ms, mfu 31.17%
iter 87: loss 3.1338, time 3145.46ms, mfu 31.49%
iter 88: loss 3.5449, time 3145.63ms, mfu 31.77%
iter 89: loss 3.2637, time 3145.19ms, mfu 32.03%
step 90: train loss 3.1533, val loss 3.3024
iter 90: loss 1.6154, time 5390.31ms, mfu 30.83%
iter 91: loss 3.7362, time 3145.55ms, mfu 31.18%
iter 92: loss 2.5064, time 3146.85ms, mfu 31.49%
iter 93: loss 4.4303, time 3145.83ms, mfu 31.77%
iter 94: loss 3.2869, time 3146.15ms, mfu 32.03%
step 95: train loss 2.9465, val loss 3.3532
iter 95: loss 1.7437, time 5390.12ms, mfu 30.83%
iter 96: loss 2.6348, time 3145.92ms, mfu 31.18%
iter 97: loss 4.1435, time 3145.18ms, mfu 31.49%
iter 98: loss 2.9146, time 3146.46ms, mfu 31.77%
iter 99: loss 3.7885, time 3145.37ms, mfu 32.03%
step 100: train loss 3.0177, val loss 3.2271
iter 100: loss 3.3526, time 5389.68ms, mfu 30.83%
iter 101: loss 3.3230, time 3145.51ms, mfu 31.18%
iter 102: loss 3.1042, time 3145.23ms, mfu 31.49%
iter 103: loss 3.2992, time 3353.72ms, mfu 31.56%
iter 104: loss 2.9468, time 3145.54ms, mfu 31.84%
step 105: train loss 3.3475, val loss 3.2328
iter 105: loss 3.0014, time 5390.21ms, mfu 30.66%
iter 106: loss 3.1059, time 3145.84ms, mfu 31.02%
iter 107: loss 3.4089, time 3145.29ms, mfu 31.35%
iter 108: loss 2.6810, time 3145.87ms, mfu 31.65%
iter 109: loss 2.9399, time 3146.11ms, mfu 31.92%
step 110: train loss 3.0261, val loss 3.2772
iter 110: loss 2.2815, time 5391.04ms, mfu 30.73%
iter 111: loss 3.2230, time 3146.02ms, mfu 31.09%
iter 112: loss 3.2744, time 3144.52ms, mfu 31.41%
iter 113: loss 3.4221, time 3151.29ms, mfu 31.70%
iter 114: loss 4.4831, time 3146.75ms, mfu 31.96%
step 115: train loss 3.3287, val loss 3.1695
iter 115: loss 2.9315, time 5528.91ms, mfu 30.71%
iter 116: loss 1.9541, time 3153.65ms, mfu 31.07%
iter 117: loss 3.9446, time 3144.05ms, mfu 31.39%
iter 118: loss 3.8799, time 3148.85ms, mfu 31.68%
iter 119: loss 3.4673, time 3147.84ms, mfu 31.94%
step 120: train loss 3.1572, val loss 3.3851
iter 120: loss 3.2654, time 5416.52ms, mfu 30.74%
iter 121: loss 3.0128, time 3148.68ms, mfu 31.10%
iter 122: loss 3.5247, time 3145.88ms, mfu 31.42%
iter 123: loss 4.1408, time 3146.78ms, mfu 31.71%
iter 124: loss 3.8952, time 3146.36ms, mfu 31.97%
step 125: train loss 3.1091, val loss 3.4129
iter 125: loss 3.8734, time 5414.05ms, mfu 30.77%
iter 126: loss 3.5337, time 3147.56ms, mfu 31.12%
iter 127: loss 3.2551, time 3146.08ms, mfu 31.44%
iter 128: loss 3.7548, time 3147.01ms, mfu 31.73%
iter 129: loss 3.5563, time 3145.42ms, mfu 31.99%
step 130: train loss 3.2390, val loss 3.4084
iter 130: loss 2.5924, time 5431.31ms, mfu 30.77%
iter 131: loss 3.7649, time 3146.03ms, mfu 31.13%
iter 132: loss 3.1821, time 3157.29ms, mfu 31.44%
iter 133: loss 3.3296, time 3145.46ms, mfu 31.72%
iter 134: loss 2.7501, time 3145.87ms, mfu 31.98%
step 135: train loss 3.0828, val loss 3.2104
iter 135: loss 3.8487, time 5390.88ms, mfu 30.79%
iter 136: loss 2.1958, time 3145.72ms, mfu 31.14%
iter 137: loss 3.4411, time 3145.60ms, mfu 31.46%
iter 138: loss 2.9261, time 3145.35ms, mfu 31.75%
iter 139: loss 3.5703, time 3145.70ms, mfu 32.00%
step 140: train loss 2.9211, val loss 3.2829
iter 140: loss 3.7226, time 5395.77ms, mfu 30.80%
iter 141: loss 3.4087, time 3145.52ms, mfu 31.16%
iter 142: loss 2.5238, time 3145.42ms, mfu 31.47%
iter 143: loss 2.6083, time 3145.40ms, mfu 31.76%
iter 144: loss 3.2717, time 3145.73ms, mfu 32.01%
step 145: train loss 3.0461, val loss 3.3649
iter 145: loss 2.9767, time 5392.43ms, mfu 30.81%
iter 146: loss 2.7207, time 3145.51ms, mfu 31.17%
iter 147: loss 1.3992, time 3145.57ms, mfu 31.48%
iter 148: loss 3.7787, time 3145.17ms, mfu 31.77%
iter 149: loss 2.5464, time 3145.09ms, mfu 32.02%
step 150: train loss 2.8755, val loss 3.3100
iter 150: loss 2.2635, time 5438.57ms, mfu 30.80%
training finished in 9m 16.41s
